---
title: "Machine Learning - LC loans"
author: "Svetlana Mikheyenoka"
date: "March 5, 2017"
output: 
  pdf_document: 
    toc: yes
    toc_depth: 4
---
## Introduction 

The dataset I used for this project was obtained from LendingClub (LC), a peer to peer lender located in San Francisco, California. The data is representative of LC's initial loan issuances from 2007 through 2011. I thought it would be interesting to look into these specific years to see if there were any significant trends in predicting whether a borrower would pay off their loan or not (paid off or charged off respectively). As I will take you through the machine learning process we will be able to see the final results which will define whether or not this may have been useful for the company to utilize. Coming from loan originating, if the company was able to identify key characteristics that reflect a common trait of an individual who will most likely charge off their loan, then the underwriters and loan analysts could do an even better job in weeding these people out. This in turn would not only increase the revenue generated by the company but considering this is a peer to peer lender, this too will benefit investors who are looking to make a consistent return. 

```{r, include=FALSE}
rm(list=ls())
```

```{r, include=FALSE}
#install.packages("ROCR")
library(ROCR)
library(descr)

setwd("/Users/smikheyenok/Desktop/CEU/DS Bus/ML")
getwd()
rawdata <- read.csv("0711LoanStats.csv", sep = ",", stringsAsFactors = FALSE,header = T,skip = 1)

```

## Exploratory Data Analysis 
 
The raw data contained `r formatC(nrow(rawdata),format="d",big.mark=",")` observations wiht 111 features. I created a subset with data including features only relevant to my response outcome bringing the total number of features down to 26. 

### Data Cleaning
```{r, echo=TRUE}
data<-subset(rawdata,select=c(loan_amnt,funded_amnt,funded_amnt_inv,term,int_rate,installment,grade,sub_grade,emp_length,home_ownership,annual_inc,verification_status,issue_d,loan_status,purpose,addr_state,dti,earliest_cr_line,inq_last_6mths,mths_since_last_delinq,open_acc,pub_rec,revol_bal,revol_util,total_acc))

data$had_delinq<-1
data[is.na(data$mths_since_last_delinq),]$had_delinq<-0
data$mths_since_last_delinq[is.na(data$mths_since_last_delinq)]<-0
```
```{r, include=FALSE}
freq(data$mths_since_last_delinq )
```


"Mthssincelastdelinq" had many N/A responses. Considering this variable is interpreted as the number of months since the borrowers last deliqnuency, these are N/A reponses which we do not want to remove as these are people with likely good credit. Instead, I created a new variabe "had_delinq" and set it to TRUE to represent the people who had ever gone delinquent, including those who were 'currently' delinquent. This way the 0 response between the N/A's and the currently delinquent borrowers was separated from one another. From doing this, I was able to see people who have a clean record without accidentally removing 26,933 entries. This produced 15,609 people who range from having their most recent delinquency almost half a year ago (120 months & up) till present day (date of collected data). With this set up, I removed actual NA observations, bringing down the observation count to 42,506. Then I went through each feature changing the amounts to purely be numeric such as removing the percentage signs as well as factors, in turn creating dummy variables. 

```{r, echo=TRUE}
data<-na.omit(data)
data$term<-as.numeric(gsub(" ","",gsub("months","",data$term)))
data$int_rate<-as.numeric(gsub("%","",data$int_rate))
data$grade<-as.factor(data$grade)
data$sub_grade<-as.factor(data$sub_grade)
data$emp_length<-as.factor(data$emp_length)
data$home_ownership<-as.factor(data$home_ownership)
data$verification_status<-as.factor(data$verification_status)
data$issue_d<-as.factor(substr(data$issue_d,5,6))

data$purpose<-as.factor(data$purpose)

data$addr_state<-as.factor(data$addr_state)
data$earliest_cr_line<-as.factor(substr(data$earliest_cr_line,5,5))

data$revol_util<-as.numeric(gsub("%","",data$revol_util))
```

Lastly, I removed all observations not inclusive of "Fully Paid" or "Charged Off", as these were the results I was in search of. The remainder of status's were removed considering on average each held only 1-5 observations. Additionally, I created a new feature, CP, indicative of whether the loan did or did not meet the credit policy requirements. As I would have expected majority of the loans: 39,697 fell into LendingClub's guidelines. The remaining 2,709 loans, totalling approximatley 6.4% were exceptions most likely made by a combination of management, capital markets and the underwriting teams. 

```{r, echo=TRUE}
data<-subset(data,!((data$loan_status=="Default")|(data$loan_status=="In Grace Period")|(data$loan_status=="Late (16-30 days)")|(data$loan_status=="Late (31-120 days)")|(data$loan_status=="Current")))
data$CP<-1
data$CP[grep("Does not",data$loan_status)]<-0
data$CP<-as.factor(data$CP)
data$had_delinq<-as.factor(data$had_delinq)

data$funded_inv <- data$funded_amnt_inv
data$earliest_line <- data$earliest_cr_line
data$inq_l6months <- data$inq_last_6mths
data$mths_sincedelinq <- data$mths_since_last_delinq

data$loan_status[grep("Does not",data$loan_status)]<-gsub("Does not meet the credit policy. Status:","",data$loan_status[grep("Does not",data$loan_status)])
data$loan_status<-as.factor(data$loan_status)
freq(data$CP)
data<-na.exclude(data)
N<-nrow(data)
set.seed(1234)

```

## The Data Explained  

###  Variables utilized in the Supervised Learning dataset 

* Loan_Amnt::: The listed amount of the loan applied for by the borrower. If at some point in time, the credit 
# department reduces the loan amount, then it will be reflected in this value.

* funded_amnt:: The total amount committed to that loan at that point in time.

* funded_inv: The total amount committed by investors for that loan at that point in time.

* term: The number of payments on the loan. Values are in months and can be either 36 or 60.

* int_rate: Interest rate on the loan 

* installment: The monhtly paymented owed by the borrower if the loan originates (when borrower is approved for the loan)

* grade: LendingClub assigned loan grade (associated with an interest rate, modeled return range, modeled default range, and origination fee. The loan grades range from "A", which is the lowest modeled risk/return, to "G", which is the highest modeled risk/return.)

* sub_grade: LendingClub assigned subgrade (incremental from A1 to G5)

* emp_length: Employment length in years. Possible values are between 0 and 10 where 0 means less than one year and 10 means ten or more years. 

* home _ownership: The home ownership status provided by the borrower during registration. Value options: RENT, OWN, MORTGAGE, OTHER.

* annual_incom: The self-reported annual income provided by the borrower during registration.

* verification_status: Indicates if the co-borrowers' joint income was verified by LendingClub, not verified, or if the income source was verified

* issue_d: The month which the loan was funded

* loan_status: Current status of the loan

* purpose: A category provided by the borrower for the loan request. 

* addr_state: The state provided by the borrower in the loan application

* dti: (debt-to-income) A ratio calculated using the borrower’s total monthly debt payments on the total debt obligations, excluding mortgage and the requested LendingClub loan, divided by the borrower’s self-reported monthly income.

* earliest_line: The month the borrower's earliest reported credit line was opened

* inq_l6mths: The number of inquiries in past 6 months (excluding auto and mortgage inquiries)

* mths_sincedelinq: The number of months since the borrower's last delinquency.

* open_acc: The number of open credit lines in the borrower's credit file.

* pub_rec: Number of derogatory public records

* revol_bal: Total credit revolving balance

* revol_util: Revolving line utilization rate, or the amount of credit the borrower is using relative to all available revolving credit.

* total_acc: The total number of credit lines currently in the borrower's credit file

* CP: Whether the originated loan fell into the guidelines of LendingClub's credit policy or not. Return options are 0 or 1. 


The graphs below are representative of all the features considered in the dataset used in the following predictive modeling. It gives a good overview of the frequency of attributes within the distribution of approved loans. Additionally, it allows us to get somewhat of a generalized picture as to the kind of borrowers LendingClub had from its starting years of 2007 - 2011. 

#### Graphs for each explained variable 

```{r, echo=TRUE}
freq(data$addr_state)
for(i in 1:ncol(data)){
  if(is.factor(data[,i])){
    freq(data[,i],main=colnames(data)[i])
    print(paste("freq(data[,",i,"],main=\"",colnames(data)[i],'")',sep=""))
  }else{
    
    hist(data[,i],main=colnames(data)[i])
    print(paste("hist(data[,",i,"],main=\"",colnames(data)[i],'")',sep=""))
  }
}
```

Some facts which can be collected from the data are as follows:

*Majority of loan amounts funded fell under the $12K mark.  

* There were approximately 2x more 36month term loans compared to 60 month term loans 

* The interest rate of loans spanned, but the peak percentages were around: 8%, 11-12% and 14%. 

* The distribution of loans indicates that per due dilligence, majority of the loans were classified to be Grade A and B loan. Indicating borrower stability. 

* California proved to be the most prominant state the borrower's resided in per their application.


## Machine Learning 

I created an 'id' sample from the total 'N' observations with 70% of that data, totalling 29,684 loans. From here, I set the training data to consitute this newly created sample and utilized the remaining 30% as the test set. 

```{r, include=FALSE}
id<-sample(1:N,0.7*N)
length(id)
d_train<-data[id,]
d_test<-data[-id,]

library(descr)
library(ggplot2)
library(ggplot2)
library(randomForest)

dim(d_train)
dim(d_test)
```

### Modeling 

#### Random Forests  

To start Random Forest, I regressed the Loan Status on all the previously cleaned features utilizing 500 trees. The number of variables tried at each split is 5. The graph represents a decreasing error rate with the increase of the number of trees for both Charged off and Fully paid loans. The confusion matrix produced shows that the Charged off classification error equals 0.3696 whereas the Fully paid classification error is only slightly less at 0.3531 

```{r, echo=TRUE}
rf <- randomForest(loan_status ~ ., data = d_train, ntree = 500)
rf
plot(rf)
```

#### RF Model Evaluation 

With the data trained, I go on to predict the probability of the test set observations and finalize the concluding outcome: Charged off or Fully Paid by indicating the class. I assigned 0.4 as the threshold of classification for the test set. The final performance result was very similar to that of the training set.

* Training Performance (AUC): 0.688 

* Test Performance (AUC): 0.686

```{r, echo=TRUE}
phat_tr <- predict(rf, type = "prob")[,"Fully Paid"]
# phat_tr
rocr_obj_tr <- prediction(phat_tr, d_train$loan_status)
plot(performance(rocr_obj_tr, "err"))           
plot(performance(rocr_obj_tr, "tpr", "fpr"))     
#performance(rocr_obj_tr, "auc")     
performance(rocr_obj_tr, "auc")@y.values[[1]] 
plot(performance(rocr_obj_tr, "tpr", "fpr"), colorize=TRUE)     



phat <- predict(rf, d_test, type = "prob")[,"Fully Paid"]
#phat
phatc <- predict(rf, d_test, type = "class")
#phatc

table(phatc,d_test$loan_status)

table(ifelse(phat>0.4,1,0), d_test$loan_status)

rocr_obj <- prediction(phat, d_test$loan_status)
plot(performance(rocr_obj, "err"))          
plot(performance(rocr_obj, "tpr", "fpr"))    
#performance(rocr_obj, "auc")     
performance(rocr_obj, "auc")@y.values[[1]] 
plot(performance(rocr_obj, "tpr", "fpr"), colorize=TRUE)    

```


#### GBM Model Evaluation 

```{r, include=FALSE}
library(survival)
library(lattice)
library(splines)
library(parallel)
library(gbm)
```

From this model, only 9.7% of people Charged off their loan from those who were predicted to fully pay their loan and 73.4% paid off their loan from those who were predicted to Charge off. 

```{r, echo=TRUE}
d_train_ynum<-d_train
d_train_ynum$loan_status <- ifelse(d_train_ynum$loan_status=="Fully Paid",1,0)
d_test_ynum<-d_test
d_test_ynum$loan_status <- ifelse(d_test_ynum$loan_status=="Fully Paid",1,0)

mdgbm <- gbm(loan_status ~ ., data = d_train_ynum, distribution = "bernoulli",
             n.trees = 500, interaction.depth = 10, shrinkage = 0.03)
mdgbm
yhat <- predict(mdgbm, d_test, n.trees = 500)

CrossTable(ifelse(yhat>1.66,1,0), d_test$loan_status,prop.chisq=F)

```

#### GBM Test 

Here, I assigned 1.66 as the threshold of classification for the test set. The final performance result proved to be slightly better than that evaluated from the Random Forests. 

* Test Performance (AUC): 0.701 

```{r, echo=TRUE}
rocr_obj <- prediction(yhat, d_test$loan_status)
plot(performance(rocr_obj, "err"))            
plot(performance(rocr_obj, "tpr", "fpr"))     
#performance(rocr_obj, "auc")     
performance(rocr_obj, "auc")@y.values[[1]] 
plot(performance(rocr_obj, "tpr", "fpr"), colorize=TRUE)  
```

```{r, include=FALSE}
# install.packages("httr")
library(httr)
# set_config(config(ssl_verifypeer = 0L))
```

#### GBM with Cross Validation

Not too much of a difference could be seen from doing cross Validation. Yet again, the crosstable produced exact results reflecting only 9.7% of people Charged off their loan from those who were predicted to fully pay their loan. However, the prediction for those that paid off their loan from those who were predicted to Charge of their loan slightly increase by 0.4% (73.8%). 

```{r, include=FALSE}
mdgbmcv <- gbm(loan_status ~ ., data = d_train_ynum, distribution = "bernoulli",
             n.trees = 500, interaction.depth = 10, shrinkage = 0.03, cv.folds = 5)

yhatcv <- predict(mdgbmcv, d_test, n.trees = 300)
```
```{r, echo=TRUE}
CrossTable(ifelse(yhatcv>1.66,1,0), d_test$loan_status,prop.chisq=F)

rocr_obj <- prediction(yhatcv, d_test$loan_status)
plot(performance(rocr_obj, "err"))            
plot(performance(rocr_obj, "tpr", "fpr"))     
#performance(rocr_obj, "auc")     
performance(rocr_obj, "auc")@y.values[[1]] 
plot(performance(rocr_obj, "tpr", "fpr"), colorize=TRUE) 
```

Once again, the model proved to increase ever so slighlty with the final performance result (AUC) equalling approximately 0.704. 

```{r, eval=FALSE, include=FALSE}
# N<-nrow(data)
# set.seed(1234)
# id<-sample(1:N,0.5*N)
# id_train<-c(1:N)[id]
# id_s2<-c(1:N)[-id]
 

# id<-sample(1:length(id_s2),0.5*length(id_s2))
 
# id_valid<-id_s2[id]
# id_test<-id_s2[-id]
# 
# 
# d_train<-data[id_train,]
# d_valid<-data[id_valid,]
# d_test<-data[id_test,]
```

#### Neural Network Model Evaluation

With neural networks I made sure to include a validation set in addition to the train and test set. After testing out a few different amounts, the concluding result I felt comfortable was with 2 hidden layers each containing 50 nodes with 50 iterations done. The model fell slightly short of the other models, reflecting: 

* Test Performance (AUC): 0.677 

```{r, include=FALSE}
library(nnet)
set.seed(1234)

# install.packages("h2o")
library(h2o)

set.seed(1234)

```
```{r, echo=TRUE}
N1<-nrow(data)
vt<-sample(1:N1,0.6*N)
d_train<-data[vt,]
d_vt<-data[-vt,]
N2<-nrow(d_vt)
t<-sample(1:N2,0.5*N2)
d_valid<-d_vt[t,]
d_test<-d_vt[-t,]
```

```{r, include=FALSE}
h2o.init(max_mem_size = "4g", nthreads = -1)

dx_train <- as.h2o(d_train)  ## uploads data to H2O
dx_train$loan_status <- as.factor(dx_train$loan_status)
dx_valid<- as.h2o(d_valid)  ## uploads data to H2O
dx_valid$loan_status <- as.factor(dx_valid$loan_status)
dx_test <- as.h2o(d_test)
dx_test$loan_status <- as.factor(dx_test$loan_status)

h2odl<-h2o.deeplearning(x=colnames(dx_train)[-14],y="loan_status",training_frame = dx_train,validation_frame = dx_valid,hidden=c(50,50),activation="Rectifier",epochs=50)
```

```{r, echo=TRUE}
h2o.auc(h2odl)
```



```{r, eval=FALSE, include=FALSE}
# h2ogbm<-h2o.gbm(x=colnames(dx_train)[-14],y="loan_status",training_frame = dx_train,validation_frame = dx_valid,nfold=5)

# h2odl
```

With a 0.9 threshold, the confusion matrix results indicated that out 807 borrowers which were predicted to charge off their loans indeed did, whereas 2572 who were predicted to charge off their loans, actually paid them off. On the other end, 4,613 borrowers who were predicted to pay off their loans did and only 490 ended up not paying off their loans from those predicted to be a good borrower.

```{r, echo=TRUE}
h2opred<-(h2o.predict(h2odl,newdata=dx_test))


h2opred<-as.data.frame(h2opred)
# table(h2opred$predict,d_test$loan_status)
nhat<-h2opred$Fully.Paid
length(d_test$loan_status)


rocr_obj <- ROCR::prediction(nhat, d_test$loan_status)
plot(performance(rocr_obj, "err"))            # err vs cutoff
#performance(rocr_obj, "auc")     
performance(rocr_obj, "auc")@y.values[[1]] 


plot(performance(rocr_obj, "tpr", "fpr"), colorize=TRUE)     # ROC curve

table(ifelse(nhat>0.9,1,0),d_test$loan_status)

```

```{r, eval=FALSE, include=FALSE}
# plot(performance(rocr_obj, "tnr", "fnr"), colorize=TRUE) 
```



## Discussion   

The final chosen model from the ones tested above is GBM with Cross Validation leading to an AUROC of 0.704.
Therefore, the charged off prediction per the given data and models has a precision of 70.4%. Although these results would be considered not bad, I would want to train and test more observations considering much of the 
data here was from when the company was starting and ramping up. Since this point LendingClub has grown exponentially with the number of annual borrowers to the iterative versions of their internal credit policy. When I started working on this project I believed that working with the first version of LC's figures would be insightful. However, with such manufactured results I would now be interested in testing the current algorithm on the newly approved borrowers along with their underlying attributes to see how it performs. If underwriters have vetted borrowers better we should see a less precise result. 

It was interesting to play around with the Neural Networks model which proved to be rather volatile. As I altered the number of hidden layers, nodes and iterations the results returned ranged between 5 to 15 percentage points below my final presented precision rate of 67.7%. This proved to me that the probable cause was overfitting as had been thoroughly discussed in each class. 

